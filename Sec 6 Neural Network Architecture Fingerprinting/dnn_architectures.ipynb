{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0rwjQ1g8V10",
        "outputId": "8738902f-a2f1-4278-d6c2-91df0b7f2d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 484kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 7.14MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, accuracy: 0.9185\n",
            "Epoch 2, accuracy: 0.9546\n",
            "Epoch 3, accuracy: 0.9698\n",
            "Epoch 4, accuracy: 0.9772\n",
            "Epoch 5, accuracy: 0.9804\n",
            "Epoch 6, accuracy: 0.9745\n",
            "Epoch 7, accuracy: 0.9792\n",
            "Epoch 8, accuracy: 0.9685\n",
            "Epoch 9, accuracy: 0.9855\n",
            "Epoch 10, accuracy: 0.9862\n",
            "Epoch 11, accuracy: 0.9852\n",
            "Epoch 12, accuracy: 0.9877\n",
            "Epoch 13, accuracy: 0.9627\n",
            "Epoch 14, accuracy: 0.9838\n",
            "Epoch 15, accuracy: 0.9877\n",
            "Epoch 16, accuracy: 0.9861\n",
            "Epoch 17, accuracy: 0.9820\n",
            "Epoch 18, accuracy: 0.9867\n",
            "Epoch 19, accuracy: 0.9889\n",
            "Epoch 20, accuracy: 0.9784\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision torch --quiet\n",
        "#LENET-5\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# -------------------------\n",
        "# Model definition\n",
        "# -------------------------\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(256, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.pool1(self.relu1(self.conv1(x)))\n",
        "        y = self.pool2(self.relu2(self.conv2(y)))\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.relu3(self.fc1(y))\n",
        "        y = self.relu4(self.fc2(y))\n",
        "        y = self.fc3(y)\n",
        "        return y\n",
        "\n",
        "# -------------------------\n",
        "# Training setup\n",
        "# -------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "batch_size = 256\n",
        "train_dataset = MNIST(root='./data', train=True, transform=ToTensor(), download=True)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "model = Model().to(device)\n",
        "sgd = SGD(model.parameters(), lr=0.1)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "all_epoch = 20\n",
        "prev_acc = 0\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "for current_epoch in range(all_epoch):\n",
        "    model.train()\n",
        "    for train_x, train_label in train_loader:\n",
        "        train_x, train_label = train_x.to(device), train_label.to(device)\n",
        "        sgd.zero_grad()\n",
        "        predict_y = model(train_x)\n",
        "        loss = loss_fn(predict_y, train_label)\n",
        "        loss.backward()\n",
        "        sgd.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_correct_num = 0\n",
        "    all_sample_num = 0\n",
        "\n",
        "    for test_x, test_label in test_loader:\n",
        "        test_x, test_label = test_x.to(device), test_label.to(device)\n",
        "        output = model(test_x)\n",
        "        prediction = torch.argmax(output, dim=1)\n",
        "        all_correct_num += (prediction == test_label).sum().item()\n",
        "        all_sample_num += test_label.size(0)\n",
        "\n",
        "    acc = all_correct_num / all_sample_num\n",
        "    print(f\"Epoch {current_epoch+1}, accuracy: {acc:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    if not os.path.isdir(\"models\"):\n",
        "        os.mkdir(\"models\")\n",
        "    torch.save(model.state_dict(), f\"models/mnist_{acc:.3f}.pth\")\n",
        "\n",
        "    # Stop if accuracy stops changing\n",
        "    if abs(acc - prev_acc) < 1e-4:\n",
        "        break\n",
        "    prev_acc = acc\n",
        "\n",
        "print(\"Training completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DdgOBXP8uto",
        "outputId": "8b3ebc30-b458-471d-fd64-8bdafcd5aa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Epoch 1/20   Accuracy: 0.1135\n",
            "Epoch 2/20   Accuracy: 0.8579\n",
            "Epoch 3/20   Accuracy: 0.9702\n",
            "Epoch 4/20   Accuracy: 0.9811\n",
            "Epoch 5/20   Accuracy: 0.9879\n",
            "Epoch 6/20   Accuracy: 0.9884\n",
            "Epoch 7/20   Accuracy: 0.9908\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "#  FULL ALEXNET TRAINING SCRIPT FOR MNIST (COLAB READY)\n",
        "# ================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ================================================================\n",
        "#  AlexNet Model (adjusted for MNIST 1Ã—224Ã—224)\n",
        "# ================================================================\n",
        "class AlexNetMNIST(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNetMNIST, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),  # changed 3â†’1 for MNIST\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# ================================================================\n",
        "#  Setup\n",
        "# ================================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "\n",
        "# Transform MNIST for AlexNet size\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),  # AlexNet size\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Model + optimizer + loss\n",
        "model = AlexNetMNIST().to(device)\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "# ================================================================\n",
        "#  Training Loop\n",
        "# ================================================================\n",
        "prev_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # ------------------------------\n",
        "    # Evaluation\n",
        "    # ------------------------------\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}   Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    if not os.path.isdir(\"models\"):\n",
        "        os.mkdir(\"models\")\n",
        "    torch.save(model.state_dict(), f\"models/alexnet_mnist_{acc:.4f}.pth\")\n",
        "\n",
        "    # Early stopping if accuracy stops improving\n",
        "    if abs(acc - prev_acc) < 1e-4:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "    prev_acc = acc\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0BUZVsKp_cm",
        "outputId": "4afd9e98-dbaa-44cc-fb5c-41a4accf8295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 18.9MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.70MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.48MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¥ Training TinyCNN\n",
            "\n",
            "Epoch 1: Train Acc = 0.9462, Test Acc = 0.9790\n",
            "Epoch 2: Train Acc = 0.9829, Test Acc = 0.9886\n",
            "Epoch 3: Train Acc = 0.9883, Test Acc = 0.9890\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# DEVICE\n",
        "# ----------------------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# TINY CNN MODEL (very fast, ~99% accuracy)\n",
        "# ----------------------------------------------------------\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # 1 â†’ 16\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 28â†’14\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1), # 16 â†’ 32\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14â†’7\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# DATASET & DATALOADER\n",
        "# ----------------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_data, batch_size=1000)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# TRAIN / TEST FUNCTIONS\n",
        "# ----------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = loss_fn(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def test_model(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# TRAINING TINYCNN\n",
        "# ----------------------------------------------------------\n",
        "model = TinyCNN().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"\\nðŸ”¥ Training TinyCNN\\n\")\n",
        "\n",
        "for epoch in range(3):   # change epochs if needed\n",
        "    train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc  = test_model(model, test_loader)\n",
        "    print(f\"Epoch {epoch+1}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsyzNH6h8Wtf",
        "outputId": "7cc6a6ea-d5e4-4058-fe77-c207021bb10a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.2661\n",
            "Epoch [2/5], Loss: 0.0842\n",
            "Epoch [3/5], Loss: 0.0642\n",
            "Epoch [4/5], Loss: 0.0524\n",
            "Epoch [5/5], Loss: 0.0440\n",
            "Accuracy on test set: 98.40%\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "#  Micro-CNN for MNIST\n",
        "#  Extremely lightweight model\n",
        "# =============================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparameters\n",
        "# -------------------------\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "lr = 0.001\n",
        "\n",
        "# -------------------------\n",
        "# MNIST Dataset\n",
        "# -------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_set = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# -------------------------\n",
        "# Micro-CNN Model\n",
        "# -------------------------\n",
        "class MicroCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MicroCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=1)  # out: 4 Ã— 28 Ã— 28\n",
        "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)  # out: 8 Ã— 28 Ã— 28\n",
        "        self.pool = nn.MaxPool2d(2)  # out: 8 Ã— 14 Ã— 14\n",
        "\n",
        "        self.fc1 = nn.Linear(8 * 14 * 14, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MicroCNN()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# -------------------------\n",
        "# Optimizer and Loss\n",
        "# -------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Testing\n",
        "# -------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Die5VrMrGgl",
        "outputId": "1408ee45-41e2-42d4-8f68-65e0f3f3bff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss = 0.3480\n",
            "Epoch 2, Loss = 0.1653\n",
            "Epoch 3, Loss = 0.1265\n",
            "Epoch 4, Loss = 0.0927\n",
            "Epoch 5, Loss = 0.0778\n",
            "Accuracy: 97.34\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_loader = DataLoader(datasets.MNIST(\"./data\", train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(datasets.MNIST(\"./data\", train=False, download=True, transform=transform), batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# ---------- Depthwise Separable Convolution ----------\n",
        "class DSConv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # depthwise conv: groups = channels\n",
        "        self.depth1 = nn.Conv2d(1, 1, kernel_size=3, padding=1, groups=1)\n",
        "        self.point1 = nn.Conv2d(1, 8, kernel_size=1)\n",
        "\n",
        "        self.depth2 = nn.Conv2d(8, 8, kernel_size=3, padding=1, groups=8)\n",
        "        self.point2 = nn.Conv2d(8, 16, kernel_size=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc = nn.Linear(16 * 14 * 14, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.point1(self.depth1(x)))\n",
        "        x = self.pool(self.relu(self.point2(self.depth2(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DSConv().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ---------- Training ----------\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for img, label in train_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ---------- Testing ----------\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        pred = model(img).argmax(1)\n",
        "        total += label.size(0)\n",
        "        correct += (pred == label).sum().item()\n",
        "\n",
        "print(\"Accuracy:\", 100 * correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le3G9qverunB",
        "outputId": "d879c4e7-4e83-4743-fab9-2e5b338ba626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss = 0.3708\n",
            "Epoch 2, Loss = 0.1344\n",
            "Epoch 3, Loss = 0.0948\n",
            "Epoch 4, Loss = 0.0750\n",
            "Epoch 5, Loss = 0.0650\n",
            "Accuracy: 97.69\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(datasets.MNIST(\"./data\", train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(datasets.MNIST(\"./data\", train=False, download=True, transform=transform), batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# ---------- Fire Block ----------\n",
        "class Fire(nn.Module):\n",
        "    def __init__(self, in_ch, squeeze_ch, expand_ch):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.Conv2d(in_ch, squeeze_ch, kernel_size=1)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_ch, expand_ch, kernel_size=1)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_ch, expand_ch, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.squeeze(x))\n",
        "        return self.relu(torch.cat([self.expand1x1(x), self.expand3x3(x)], 1))\n",
        "\n",
        "\n",
        "# ---------- SqueezeTiny ----------\n",
        "class SqueezeTiny(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fire1 = Fire(1, 4, 4)   # Output: 8 channels\n",
        "        self.fire2 = Fire(8, 4, 4)   # Output: 8 channels\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.fc = nn.Linear(8 * 14 * 14, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fire1(x)\n",
        "        x = self.pool(self.fire2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SqueezeTiny().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ---------- Training ----------\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for img, label in train_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ---------- Testing ----------\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        pred = model(img).argmax(1)\n",
        "        total += label.size(0)\n",
        "        correct += (pred == label).sum().item()\n",
        "\n",
        "print(\"Accuracy:\", 100 * correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivkfqov-sjJe",
        "outputId": "30c78d30-2742-4b59-fb08-9a8e50a86d15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss = 0.1516\n",
            "Epoch 2, Loss = 0.0608\n",
            "Epoch 3, Loss = 0.0482\n",
            "Epoch 4, Loss = 0.0407\n",
            "Epoch 5, Loss = 0.0344\n",
            "Accuracy: 98.43\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(datasets.MNIST(\"./data\", train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(datasets.MNIST(\"./data\", train=False, download=True, transform=transform), batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# ---------- Residual Block ----------\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(ch)\n",
        "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        return self.relu(x + identity)\n",
        "\n",
        "\n",
        "# ---------- TinyResNet ----------\n",
        "class TinyResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(1, 8, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.block1 = ResBlock(8)\n",
        "        self.block2 = ResBlock(8)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(8 * 14 * 14, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv(x))\n",
        "        x = self.block1(x)\n",
        "        x = self.pool(self.block2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TinyResNet().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ---------- Training ----------\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for img, label in train_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ---------- Testing ----------\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        pred = model(img).argmax(1)\n",
        "        total += label.size(0)\n",
        "        correct += (pred == label).sum().item()\n",
        "\n",
        "print(\"Accuracy:\", 100 * correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SlTcr1VtSu6",
        "outputId": "38d69826-a864-4b05-f7e0-db32ff52a285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 0.1504\n",
            "Epoch 2: Loss = 0.0438\n",
            "Epoch 3: Loss = 0.0296\n",
            "Epoch 4: Loss = 0.0213\n",
            "Epoch 5: Loss = 0.0168\n",
            "Accuracy: 99.04\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# MNIST Dataset\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(\"./data\", train=True, download=True, transform=transform),\n",
        "    batch_size=64, shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(\"./data\", train=False, download=True, transform=transform),\n",
        "    batch_size=64, shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Inception Block (Tiny)\n",
        "# ---------------------------\n",
        "class InceptionTiny(nn.Module):\n",
        "    def __init__(self, in_ch, out_1x1, out_3x3, out_5x5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_1x1, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_3x3, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch5 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_5x5, kernel_size=5, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.branch1(x),\n",
        "                          self.branch3(x),\n",
        "                          self.branch5(x)], dim=1)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Tiny GoogLeNet for MNIST\n",
        "# ---------------------------\n",
        "class TinyGoogLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # One tiny inception block â€” outputs: 8 + 8 + 8 = 24 channels\n",
        "        self.inception1 = InceptionTiny(16, 8, 8, 8)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)  # 28 â†’ 14\n",
        "\n",
        "        # Second tiny inception block\n",
        "        self.inception2 = InceptionTiny(24, 12, 12, 12)  # outputs 36\n",
        "\n",
        "        self.fc = nn.Linear(36 * 14 * 14, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inception1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.inception2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training Setup\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TinyGoogLeNet().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training Loop\n",
        "# ---------------------------\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for img, label in train_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img)\n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Test Accuracy\n",
        "# ---------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img, label = img.to(device), label.to(device)\n",
        "        pred = model(img).argmax(1)\n",
        "        correct += (pred == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "print(\"Accuracy:\", 100 * correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VEi1bHeuxos",
        "outputId": "a4968b66-be64-43b4-bf75-6d0954ee7a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDevice: cpu\n",
            "\n",
            "Available models:\n",
            " - nanocnn\n",
            " - picocnn\n",
            " - minimobilenet\n",
            " - mobilenano\n",
            " - tinyshufflenet\n",
            " - microdensenet\n",
            " - tinyeffnet\n",
            " - firenetmini\n"
          ]
        }
      ],
      "source": [
        "# Colab-ready: All 8 ultra-lightweight MNIST models in one notebook\n",
        "# Run this cell start-to-finish.\n",
        "\n",
        "!pip install -q torch torchvision --upgrade\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "from functools import reduce\n",
        "import operator\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# Hyperparams (tweak if needed)\n",
        "# --------------------------\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 5\n",
        "LR = 1e-3\n",
        "\n",
        "# --------------------------\n",
        "# MNIST dataset (28x28 grayscale)\n",
        "# --------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------\n",
        "# Utility: count params\n",
        "# --------------------------\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# --------------------------\n",
        "# 1) NanoCNN (~20K params)\n",
        "# --------------------------\n",
        "class NanoCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1),  # 1->8, 28x28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                # 8x14x14\n",
        "            nn.Conv2d(8, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                # 16x7x7\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16*7*7, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# --------------------------\n",
        "# 2) PicoCNN (~8K params)\n",
        "# --------------------------\n",
        "class PicoCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),    # 6x14x14\n",
        "            nn.Conv2d(6, 12, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),    # 12x7x7\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(12*7*7, 40),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(40, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --------------------------\n",
        "# 3) MiniMobileNet (simple DSConv blocks)\n",
        "# --------------------------\n",
        "def depthwise_conv(in_ch, out_ch, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, in_ch, kernel_size=3, stride=stride, padding=1, groups=in_ch, bias=False),\n",
        "        nn.BatchNorm2d(in_ch),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "class MiniMobileNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1, bias=False), nn.BatchNorm2d(8), nn.ReLU()\n",
        "        )\n",
        "        self.ds1 = depthwise_conv(8, 16, stride=1)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.ds2 = depthwise_conv(16, 24, stride=1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(24*7*7, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.ds1(x)\n",
        "        x = self.pool(x)   # 16x14x14\n",
        "        x = self.ds2(x)    # 24x14x14\n",
        "        x = self.pool(x)   # 24x7x7\n",
        "        return self.fc(x)\n",
        "\n",
        "# --------------------------\n",
        "# 4) MobileNet-Nano (smaller channels)\n",
        "# --------------------------\n",
        "class MobileNetNano(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.dw_pw = nn.Sequential(\n",
        "            nn.Conv2d(6, 6, 3, padding=1, groups=6, bias=False),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(6, 12, 1, bias=False),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2), # 12x14x14\n",
        "\n",
        "            nn.Conv2d(12, 12, 3, padding=1, groups=12, bias=False),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12, 20, 1, bias=False),\n",
        "            nn.BatchNorm2d(20),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)  # 20x7x7\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(20*7*7, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.dw_pw(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# --------------------------\n",
        "# 5) TinyShuffleNet (simple channel shuffle block)\n",
        "# --------------------------\n",
        "def channel_shuffle(x, groups):\n",
        "    batchsize, num_channels, height, width = x.size()\n",
        "    channels_per_group = num_channels // groups\n",
        "    # reshape\n",
        "    x = x.view(batchsize, groups, channels_per_group, height, width)\n",
        "    x = torch.transpose(x, 1, 2).contiguous()\n",
        "    x = x.view(batchsize, -1, height, width)\n",
        "    return x\n",
        "\n",
        "class ShuffleUnit(nn.Module):\n",
        "    def __init__(self, in_c, out_c, groups=2):\n",
        "        super().__init__()\n",
        "        mid_c = out_c // 2\n",
        "        self.groups = groups\n",
        "        self.conv_reduce = nn.Conv2d(in_c, mid_c, 1, bias=False)\n",
        "        self.dw = nn.Conv2d(mid_c, mid_c, 3, padding=1, groups=mid_c, bias=False)\n",
        "        self.conv_expand = nn.Conv2d(mid_c, out_c, 1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_reduce(x)\n",
        "        x = self.dw(x)\n",
        "        x = self.conv_expand(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = channel_shuffle(x, groups=self.groups)\n",
        "        return x\n",
        "\n",
        "class TinyShuffleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1, 12, 3, padding=1)\n",
        "        self.stage1 = ShuffleUnit(12, 24)\n",
        "        self.pool  = nn.MaxPool2d(2)  # 24x14x14\n",
        "        self.stage2 = ShuffleUnit(24, 32)\n",
        "        self.pool2  = nn.MaxPool2d(2) # 32x7x7\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(32*7*7, 64), nn.ReLU(), nn.Linear(64, 10))\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# --------------------------\n",
        "# 6) MicroDenseNet (very small DenseNet-like)\n",
        "# --------------------------\n",
        "class TinyDenseLayer(nn.Module):\n",
        "    def __init__(self, in_ch, growth):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_ch)\n",
        "        self.conv = nn.Conv2d(in_ch, growth, 3, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        return torch.cat([x, out], 1)\n",
        "\n",
        "class MicroDenseNet(nn.Module):\n",
        "    def __init__(self, growth=8):\n",
        "        super().__init__()\n",
        "        self.init = nn.Conv2d(1, 12, 3, padding=1, bias=False)  # 12\n",
        "        # dense block 1 (3 layers)\n",
        "        self.d1_1 = TinyDenseLayer(12, growth)\n",
        "        self.d1_2 = TinyDenseLayer(12+growth, growth)\n",
        "        self.d1_3 = TinyDenseLayer(12+2*growth, growth)\n",
        "        self.trans1 = nn.Sequential(nn.Conv2d(12+3*growth, 20, 1, bias=False), nn.AvgPool2d(2))\n",
        "        # dense block 2 (2 layers)\n",
        "        self.d2_1 = TinyDenseLayer(20, growth)\n",
        "        self.d2_2 = TinyDenseLayer(20+growth, growth)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(20+2*growth, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.init(x)\n",
        "        x = self.d1_1(x)\n",
        "        x = self.d1_2(x)\n",
        "        x = self.d1_3(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.d2_1(x)\n",
        "        x = self.d2_2(x)\n",
        "        x = self.pool(x).view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# --------------------------\n",
        "# 7) TinyEffNet (mini MBConv with expansion)\n",
        "# --------------------------\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, exp=4, stride=1):\n",
        "        super().__init__()\n",
        "        mid = in_ch * exp\n",
        "        self.expand = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid, 1, bias=False),\n",
        "            nn.BatchNorm2d(mid),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.dw = nn.Sequential(\n",
        "            nn.Conv2d(mid, mid, 3, stride=stride, padding=1, groups=mid, bias=False),\n",
        "            nn.BatchNorm2d(mid),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(mid, out_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch)\n",
        "        )\n",
        "        self.use_res = (in_ch == out_ch and stride == 1)\n",
        "    def forward(self, x):\n",
        "        out = self.expand(x)\n",
        "        out = self.dw(out)\n",
        "        out = self.project(out)\n",
        "        if self.use_res:\n",
        "            return out + x\n",
        "        return out\n",
        "\n",
        "class TinyEffNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1, 8, 3, padding=1, bias=False)\n",
        "        self.mb1 = MBConv(8, 16, exp=2, stride=1)\n",
        "        self.pool = nn.MaxPool2d(2)   # 16x14x14\n",
        "        self.mb2 = MBConv(16, 24, exp=2, stride=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)  # 24x7x7\n",
        "        self.head = nn.Sequential(nn.Flatten(), nn.Linear(24*7*7, 64), nn.ReLU(), nn.Linear(64, 10))\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.mb1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.mb2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# --------------------------\n",
        "# 8) FireNet-Mini (mini SqueezeNet Fire)\n",
        "# --------------------------\n",
        "class Fire(nn.Module):\n",
        "    def __init__(self, in_ch, squeeze_ch, expand_ch):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.Conv2d(in_ch, squeeze_ch, 1)\n",
        "        self.expand1 = nn.Conv2d(squeeze_ch, expand_ch, 1)\n",
        "        self.expand3 = nn.Conv2d(squeeze_ch, expand_ch, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        s = self.relu(self.squeeze(x))\n",
        "        e = torch.cat([self.relu(self.expand1(s)), self.relu(self.expand3(s))], dim=1)\n",
        "        return e\n",
        "\n",
        "class FireNetMini(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1, 8, 3, padding=1)\n",
        "        self.fire1 = Fire(8, 4, 4)   # -> 8\n",
        "        self.pool = nn.MaxPool2d(2)  # 8x14x14\n",
        "        self.fire2 = Fire(8, 6, 6)   # -> 12\n",
        "        self.pool2 = nn.MaxPool2d(2) # -> 12x7x7\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(12*7*7, 64), nn.ReLU(), nn.Linear(64, 10))\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.fire1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.fire2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# --------------------------\n",
        "# Model factory\n",
        "# --------------------------\n",
        "MODEL_REGISTRY = {\n",
        "    \"nanocnn\": NanoCNN,\n",
        "    \"picocnn\": PicoCNN,\n",
        "    \"minimobilenet\": MiniMobileNet,\n",
        "    \"mobilenano\": MobileNetNano,\n",
        "    \"tinyshufflenet\": TinyShuffleNet,\n",
        "    \"microdensenet\": MicroDenseNet,\n",
        "    \"tinyeffnet\": TinyEffNet,\n",
        "    \"firenetmini\": FireNetMini\n",
        "}\n",
        "\n",
        "def make_model(name):\n",
        "    assert name in MODEL_REGISTRY, f\"Unknown model {name}\"\n",
        "    return MODEL_REGISTRY[name]()\n",
        "\n",
        "# --------------------------\n",
        "# Train & Eval helpers\n",
        "# --------------------------\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    n = 0\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X.size(0)\n",
        "        preds = out.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        n += X.size(0)\n",
        "    return total_loss / n, correct / n\n",
        "\n",
        "def evaluate(model, loader, loss_fn=None):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            out = model(X)\n",
        "            if loss_fn:\n",
        "                losses += loss_fn(out, y).item() * X.size(0)\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += X.size(0)\n",
        "    return (losses / total) if loss_fn else None, correct / total\n",
        "\n",
        "# --------------------------\n",
        "# Run / orchestrate\n",
        "# --------------------------\n",
        "def run_model(name, epochs=EPOCHS, verbose=True):\n",
        "    model = make_model(name).to(device)\n",
        "    p = count_params(model)\n",
        "    print(f\"\\n=== Model: {name} | params = {p:,}\")\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "    t0 = time.time()\n",
        "    for ep in range(1, epochs+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, loss_fn)\n",
        "        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_loss\"].append(val_loss); history[\"val_acc\"].append(val_acc)\n",
        "        if verbose:\n",
        "            print(f\"Ep {ep}/{epochs} | train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Finished {name} in {dt:.1f}s | final val_acc={history['val_acc'][-1]:.4f}\")\n",
        "    return model, history\n",
        "\n",
        "# --------------------------\n",
        "# Interactive: choose models to run\n",
        "# --------------------------\n",
        "print(\"\\nAvailable models:\")\n",
        "for k in MODEL_REGISTRY.keys():\n",
        "    print(\" -\", k)\n",
        "\n",
        "# Example usage:\n",
        "# 1) Run a single model:\n",
        "# model, hist = run_model(\"nanocnn\", epochs=3)\n",
        "#\n",
        "# 2) Run all models sequentially (careful: will take longer):\n",
        "# results = {}\n",
        "# for key in MODEL_REGISTRY.keys():\n",
        "#     m, h = run_model(key, epochs=2)\n",
        "#     results[key] = (m, h)\n",
        "#\n",
        "# Uncomment below to run all with 2 epochs each (recommended if you want quick comparison):\n",
        "#\n",
        "#results = {}\n",
        "#for key in MODEL_REGISTRY.keys():\n",
        "#    results[key] = run_model(key, epochs=2)\n",
        "#\n",
        "# Or choose one model to run now:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t5VTUvFuz29",
        "outputId": "1f0eccde-9440-4338-e6f7-29f7a778c534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Model: nanocnn | params = 52,138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1/5 | train_loss=0.3154 train_acc=0.9068 | val_loss=0.0986 val_acc=0.9686\n",
            "Ep 2/5 | train_loss=0.0859 train_acc=0.9747 | val_loss=0.0618 val_acc=0.9805\n",
            "Ep 3/5 | train_loss=0.0628 train_acc=0.9811 | val_loss=0.0572 val_acc=0.9814\n",
            "Ep 4/5 | train_loss=0.0505 train_acc=0.9846 | val_loss=0.0421 val_acc=0.9863\n",
            "Ep 5/5 | train_loss=0.0411 train_acc=0.9878 | val_loss=0.0375 val_acc=0.9868\n",
            "Finished nanocnn in 149.3s | final val_acc=0.9868\n"
          ]
        }
      ],
      "source": [
        "selected = \"nanocnn\"   # change to any name above, e.g. \"microdensenet\"\n",
        "model, history = run_model(selected, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBrGpqrsyxHt",
        "outputId": "c727f498-8f39-4a05-fb36-bc5fce8d9b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "\n",
            "=== Training nanocnn ===\n",
            "Params: 52,138\n",
            "Epoch 1/10: train_acc=0.9121, val_acc=0.9690\n",
            "Epoch 2/10: train_acc=0.9717, val_acc=0.9773\n",
            "Epoch 3/10: train_acc=0.9804, val_acc=0.9819\n",
            "Epoch 4/10: train_acc=0.9846, val_acc=0.9848\n",
            "Epoch 5/10: train_acc=0.9860, val_acc=0.9845\n",
            "Epoch 6/10: train_acc=0.9888, val_acc=0.9827\n",
            "Epoch 7/10: train_acc=0.9898, val_acc=0.9864\n",
            "Epoch 8/10: train_acc=0.9906, val_acc=0.9862\n",
            "Epoch 9/10: train_acc=0.9916, val_acc=0.9867\n",
            "Epoch 10/10: train_acc=0.9934, val_acc=0.9871\n",
            "Final validation accuracy for nanocnn: 0.9871\n",
            "Time elapsed: 281.68 sec\n",
            "\n",
            "\n",
            "=== Training picocnn ===\n",
            "Params: 24,690\n",
            "Epoch 1/10: train_acc=0.8889, val_acc=0.9620\n",
            "Epoch 2/10: train_acc=0.9681, val_acc=0.9768\n",
            "Epoch 3/10: train_acc=0.9769, val_acc=0.9815\n",
            "Epoch 4/10: train_acc=0.9804, val_acc=0.9813\n",
            "Epoch 5/10: train_acc=0.9828, val_acc=0.9845\n",
            "Epoch 6/10: train_acc=0.9848, val_acc=0.9836\n",
            "Epoch 7/10: train_acc=0.9867, val_acc=0.9858\n",
            "Epoch 8/10: train_acc=0.9880, val_acc=0.9842\n",
            "Epoch 9/10: train_acc=0.9891, val_acc=0.9864\n",
            "Epoch 10/10: train_acc=0.9901, val_acc=0.9862\n",
            "Final validation accuracy for picocnn: 0.9862\n",
            "Time elapsed: 272.49 sec\n",
            "\n",
            "\n",
            "=== Training minimobilenet ===\n",
            "Params: 76,922\n",
            "Epoch 1/10: train_acc=0.9468, val_acc=0.9747\n",
            "Epoch 2/10: train_acc=0.9808, val_acc=0.9834\n",
            "Epoch 3/10: train_acc=0.9866, val_acc=0.9821\n",
            "Epoch 4/10: train_acc=0.9887, val_acc=0.9842\n",
            "Epoch 5/10: train_acc=0.9906, val_acc=0.9883\n",
            "Epoch 6/10: train_acc=0.9922, val_acc=0.9850\n",
            "Epoch 7/10: train_acc=0.9924, val_acc=0.9827\n",
            "Epoch 8/10: train_acc=0.9942, val_acc=0.9863\n",
            "Epoch 9/10: train_acc=0.9951, val_acc=0.9890\n",
            "Epoch 10/10: train_acc=0.9959, val_acc=0.9873\n",
            "Final validation accuracy for minimobilenet: 0.9873\n",
            "Time elapsed: 597.71 sec\n",
            "\n",
            "\n",
            "=== Training mobilenano ===\n",
            "Params: 76,922\n",
            "Epoch 1/10: train_acc=0.9413, val_acc=0.9783\n",
            "Epoch 2/10: train_acc=0.9799, val_acc=0.9833\n",
            "Epoch 3/10: train_acc=0.9850, val_acc=0.9856\n",
            "Epoch 4/10: train_acc=0.9879, val_acc=0.9853\n",
            "Epoch 5/10: train_acc=0.9902, val_acc=0.9846\n",
            "Epoch 6/10: train_acc=0.9911, val_acc=0.9863\n",
            "Epoch 7/10: train_acc=0.9927, val_acc=0.9855\n",
            "Epoch 8/10: train_acc=0.9940, val_acc=0.9891\n",
            "Epoch 9/10: train_acc=0.9944, val_acc=0.9852\n",
            "Epoch 10/10: train_acc=0.9956, val_acc=0.9887\n",
            "Final validation accuracy for mobilenano: 0.9887\n",
            "Time elapsed: 592.66 sec\n",
            "\n",
            "\n",
            "=== Training tinyshufflenet ===\n",
            "Params: 102,878\n",
            "Epoch 1/10: train_acc=0.9506, val_acc=0.9807\n",
            "Epoch 2/10: train_acc=0.9828, val_acc=0.9844\n",
            "Epoch 3/10: train_acc=0.9865, val_acc=0.9860\n",
            "Epoch 4/10: train_acc=0.9889, val_acc=0.9872\n",
            "Epoch 5/10: train_acc=0.9906, val_acc=0.9883\n",
            "Epoch 6/10: train_acc=0.9919, val_acc=0.9889\n",
            "Epoch 7/10: train_acc=0.9928, val_acc=0.9901\n",
            "Epoch 8/10: train_acc=0.9935, val_acc=0.9903\n",
            "Epoch 9/10: train_acc=0.9942, val_acc=0.9889\n",
            "Epoch 10/10: train_acc=0.9952, val_acc=0.9889\n",
            "Final validation accuracy for tinyshufflenet: 0.9889\n",
            "Time elapsed: 849.56 sec\n",
            "\n",
            "\n",
            "=== Training microdensenet ===\n",
            "Params: 9,190\n",
            "Epoch 1/10: train_acc=0.5760, val_acc=0.7968\n",
            "Epoch 2/10: train_acc=0.8712, val_acc=0.8362\n",
            "Epoch 3/10: train_acc=0.9233, val_acc=0.8771\n",
            "Epoch 4/10: train_acc=0.9410, val_acc=0.9485\n",
            "Epoch 5/10: train_acc=0.9493, val_acc=0.8972\n",
            "Epoch 6/10: train_acc=0.9542, val_acc=0.9332\n",
            "Epoch 7/10: train_acc=0.9590, val_acc=0.9544\n",
            "Epoch 8/10: train_acc=0.9628, val_acc=0.9230\n",
            "Epoch 9/10: train_acc=0.9664, val_acc=0.9669\n",
            "Epoch 10/10: train_acc=0.9689, val_acc=0.9665\n",
            "Final validation accuracy for microdensenet: 0.9665\n",
            "Time elapsed: 1613.25 sec\n",
            "\n",
            "\n",
            "=== Training tinyeffnet ===\n",
            "Params: 78,418\n",
            "Epoch 1/10: train_acc=0.9452, val_acc=0.9810\n",
            "Epoch 2/10: train_acc=0.9836, val_acc=0.9860\n",
            "Epoch 3/10: train_acc=0.9880, val_acc=0.9838\n",
            "Epoch 4/10: train_acc=0.9908, val_acc=0.9869\n",
            "Epoch 5/10: train_acc=0.9926, val_acc=0.9876\n",
            "Epoch 6/10: train_acc=0.9938, val_acc=0.9882\n",
            "Epoch 7/10: train_acc=0.9949, val_acc=0.9888\n",
            "Epoch 8/10: train_acc=0.9958, val_acc=0.9896\n",
            "Epoch 9/10: train_acc=0.9951, val_acc=0.9856\n",
            "Epoch 10/10: train_acc=0.9966, val_acc=0.9901\n",
            "Final validation accuracy for tinyeffnet: 0.9901\n",
            "Time elapsed: 851.53 sec\n",
            "\n",
            "\n",
            "=== Training firenetmini ===\n",
            "Params: 39,056\n",
            "Epoch 1/10: train_acc=0.8619, val_acc=0.9556\n",
            "Epoch 2/10: train_acc=0.9572, val_acc=0.9619\n",
            "Epoch 3/10: train_acc=0.9674, val_acc=0.9748\n",
            "Epoch 4/10: train_acc=0.9729, val_acc=0.9764\n",
            "Epoch 5/10: train_acc=0.9770, val_acc=0.9795\n",
            "Epoch 6/10: train_acc=0.9792, val_acc=0.9801\n",
            "Epoch 7/10: train_acc=0.9822, val_acc=0.9814\n",
            "Epoch 8/10: train_acc=0.9840, val_acc=0.9840\n",
            "Epoch 9/10: train_acc=0.9852, val_acc=0.9849\n",
            "Epoch 10/10: train_acc=0.9873, val_acc=0.9840\n",
            "Final validation accuracy for firenetmini: 0.9840\n",
            "Time elapsed: 416.86 sec\n",
            "\n",
            "\n",
            "=== Training tinycnn ===\n",
            "Params: 44,426\n",
            "Epoch 1/10: train_acc=0.9057, val_acc=0.9681\n",
            "Epoch 2/10: train_acc=0.9740, val_acc=0.9800\n",
            "Epoch 3/10: train_acc=0.9809, val_acc=0.9817\n",
            "Epoch 4/10: train_acc=0.9850, val_acc=0.9878\n",
            "Epoch 5/10: train_acc=0.9870, val_acc=0.9868\n",
            "Epoch 6/10: train_acc=0.9889, val_acc=0.9899\n",
            "Epoch 7/10: train_acc=0.9910, val_acc=0.9893\n",
            "Epoch 8/10: train_acc=0.9917, val_acc=0.9893\n",
            "Epoch 9/10: train_acc=0.9927, val_acc=0.9883\n",
            "Epoch 10/10: train_acc=0.9931, val_acc=0.9900\n",
            "Final validation accuracy for tinycnn: 0.9900\n",
            "Time elapsed: 259.07 sec\n",
            "\n",
            "\n",
            "=== Training microcnn ===\n",
            "Params: 7,098\n",
            "Epoch 1/10: train_acc=0.8519, val_acc=0.9537\n",
            "Epoch 2/10: train_acc=0.9572, val_acc=0.9684\n",
            "Epoch 3/10: train_acc=0.9677, val_acc=0.9709\n",
            "Epoch 4/10: train_acc=0.9729, val_acc=0.9756\n",
            "Epoch 5/10: train_acc=0.9756, val_acc=0.9783\n",
            "Epoch 6/10: train_acc=0.9771, val_acc=0.9755\n",
            "Epoch 7/10: train_acc=0.9790, val_acc=0.9785\n",
            "Epoch 8/10: train_acc=0.9809, val_acc=0.9788\n",
            "Epoch 9/10: train_acc=0.9819, val_acc=0.9822\n",
            "Epoch 10/10: train_acc=0.9829, val_acc=0.9831\n",
            "Final validation accuracy for microcnn: 0.9831\n",
            "Time elapsed: 228.39 sec\n",
            "\n",
            "\n",
            "=== Training dsconv ===\n",
            "Params: 15,810\n",
            "Epoch 1/10: train_acc=0.8578, val_acc=0.9380\n",
            "Epoch 2/10: train_acc=0.9463, val_acc=0.9604\n",
            "Epoch 3/10: train_acc=0.9624, val_acc=0.9665\n",
            "Epoch 4/10: train_acc=0.9693, val_acc=0.9703\n",
            "Epoch 5/10: train_acc=0.9734, val_acc=0.9731\n",
            "Epoch 6/10: train_acc=0.9750, val_acc=0.9746\n",
            "Epoch 7/10: train_acc=0.9777, val_acc=0.9743\n",
            "Epoch 8/10: train_acc=0.9789, val_acc=0.9757\n",
            "Epoch 9/10: train_acc=0.9812, val_acc=0.9754\n",
            "Epoch 10/10: train_acc=0.9820, val_acc=0.9773\n",
            "Final validation accuracy for dsconv: 0.9773\n",
            "Time elapsed: 288.72 sec\n",
            "\n",
            "\n",
            "=== Training squeezetiny ===\n",
            "Params: 15,994\n",
            "Epoch 1/10: train_acc=0.8914, val_acc=0.9465\n",
            "Epoch 2/10: train_acc=0.9506, val_acc=0.9588\n",
            "Epoch 3/10: train_acc=0.9619, val_acc=0.9644\n",
            "Epoch 4/10: train_acc=0.9665, val_acc=0.9685\n",
            "Epoch 5/10: train_acc=0.9705, val_acc=0.9654\n",
            "Epoch 6/10: train_acc=0.9734, val_acc=0.9712\n",
            "Epoch 7/10: train_acc=0.9754, val_acc=0.9733\n",
            "Epoch 8/10: train_acc=0.9780, val_acc=0.9744\n",
            "Epoch 9/10: train_acc=0.9799, val_acc=0.9742\n",
            "Epoch 10/10: train_acc=0.9811, val_acc=0.9756\n",
            "Final validation accuracy for squeezetiny: 0.9756\n",
            "Time elapsed: 266.07 sec\n",
            "\n",
            "\n",
            "=== Training tinyresnet ===\n",
            "Params: 6,410\n",
            "Epoch 1/10: train_acc=0.9264, val_acc=0.9728\n",
            "Epoch 2/10: train_acc=0.9781, val_acc=0.9837\n",
            "Epoch 3/10: train_acc=0.9843, val_acc=0.9836\n",
            "Epoch 4/10: train_acc=0.9871, val_acc=0.9872\n",
            "Epoch 5/10: train_acc=0.9883, val_acc=0.9877\n",
            "Epoch 6/10: train_acc=0.9892, val_acc=0.9853\n",
            "Epoch 7/10: train_acc=0.9898, val_acc=0.9896\n",
            "Epoch 8/10: train_acc=0.9906, val_acc=0.9890\n",
            "Epoch 9/10: train_acc=0.9919, val_acc=0.9894\n",
            "Epoch 10/10: train_acc=0.9922, val_acc=0.9892\n",
            "Final validation accuracy for tinyresnet: 0.9892\n",
            "Time elapsed: 509.52 sec\n",
            "\n",
            "\n",
            "=== Training tinygooglenet ===\n",
            "Params: 85,350\n",
            "Epoch 1/10: train_acc=0.9522, val_acc=0.9834\n",
            "Epoch 2/10: train_acc=0.9865, val_acc=0.9878\n",
            "Epoch 3/10: train_acc=0.9905, val_acc=0.9896\n",
            "Epoch 4/10: train_acc=0.9923, val_acc=0.9889\n",
            "Epoch 5/10: train_acc=0.9949, val_acc=0.9896\n",
            "Epoch 6/10: train_acc=0.9954, val_acc=0.9902\n",
            "Epoch 7/10: train_acc=0.9966, val_acc=0.9882\n",
            "Epoch 8/10: train_acc=0.9961, val_acc=0.9874\n",
            "Epoch 9/10: train_acc=0.9973, val_acc=0.9858\n",
            "Epoch 10/10: train_acc=0.9982, val_acc=0.9891\n",
            "Final validation accuracy for tinygooglenet: 0.9891\n",
            "Time elapsed: 1137.23 sec\n",
            "\n",
            "\n",
            "=== Training lenet ===\n",
            "Params: 44,426\n",
            "Epoch 1/10: train_acc=0.9018, val_acc=0.9700\n",
            "Epoch 2/10: train_acc=0.9759, val_acc=0.9820\n",
            "Epoch 3/10: train_acc=0.9828, val_acc=0.9846\n",
            "Epoch 4/10: train_acc=0.9859, val_acc=0.9849\n",
            "Epoch 5/10: train_acc=0.9880, val_acc=0.9860\n",
            "Epoch 6/10: train_acc=0.9900, val_acc=0.9891\n",
            "Epoch 7/10: train_acc=0.9918, val_acc=0.9883\n",
            "Epoch 8/10: train_acc=0.9928, val_acc=0.9893\n",
            "Epoch 9/10: train_acc=0.9930, val_acc=0.9891\n",
            "Epoch 10/10: train_acc=0.9945, val_acc=0.9901\n",
            "Final validation accuracy for lenet: 0.9901\n",
            "Time elapsed: 261.59 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Colab-ready: Train 14 lightweight CNN models on MNIST for 10 epochs each\n",
        "!pip install -q torch torchvision --upgrade\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# Hyperparameters\n",
        "# --------------------------\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "# --------------------------\n",
        "# MNIST Dataset\n",
        "# --------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------\n",
        "# Utility: count parameters\n",
        "# --------------------------\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# --------------------------\n",
        "# Model Definitions\n",
        "# --------------------------\n",
        "\n",
        "# 1) NanoCNN\n",
        "class NanoCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(8, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16*7*7, 64), nn.ReLU(), nn.Linear(64, 10)\n",
        "        )\n",
        "    def forward(self, x): return self.classifier(self.features(x))\n",
        "\n",
        "# 2) PicoCNN\n",
        "class PicoCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,6,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(6,12,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(12*7*7,40), nn.ReLU(),\n",
        "            nn.Linear(40,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# 3) MiniMobileNet\n",
        "def depthwise_conv(in_ch,out_ch,stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch,in_ch,3,stride=stride,padding=1,groups=in_ch,bias=False),\n",
        "        nn.BatchNorm2d(in_ch), nn.ReLU(),\n",
        "        nn.Conv2d(in_ch,out_ch,1,bias=False),\n",
        "        nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "    )\n",
        "class MiniMobileNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(nn.Conv2d(1,8,3,padding=1,bias=False), nn.BatchNorm2d(8), nn.ReLU())\n",
        "        self.ds1 = depthwise_conv(8,16)\n",
        "        self.ds2 = depthwise_conv(16,24)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(24*7*7,64), nn.ReLU(), nn.Linear(64,10))\n",
        "    def forward(self,x):\n",
        "        x = self.stem(x)\n",
        "        x = self.ds1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.ds2(x)\n",
        "        x = self.pool(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 4) MobileNet-Nano (smaller version)\n",
        "class MobileNetNano(MiniMobileNet): pass\n",
        "\n",
        "# 5) TinyShuffleNet\n",
        "def channel_shuffle(x,groups):\n",
        "    b,c,h,w = x.size()\n",
        "    cpg = c // groups\n",
        "    x = x.view(b,groups,cpg,h,w)\n",
        "    x = x.transpose(1,2).contiguous()\n",
        "    return x.view(b,-1,h,w)\n",
        "class ShuffleUnit(nn.Module):\n",
        "    def __init__(self,in_c,out_c,groups=2):\n",
        "        super().__init__()\n",
        "        mid_c = out_c//2\n",
        "        self.conv_reduce = nn.Conv2d(in_c,mid_c,1,bias=False)\n",
        "        self.dw = nn.Conv2d(mid_c,mid_c,3,padding=1,groups=mid_c,bias=False)\n",
        "        self.conv_expand = nn.Conv2d(mid_c,out_c,1,bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.groups = groups\n",
        "    def forward(self,x):\n",
        "        x = self.conv_reduce(x)\n",
        "        x = self.dw(x)\n",
        "        x = self.conv_expand(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return channel_shuffle(x,self.groups)\n",
        "class TinyShuffleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1,12,3,padding=1)\n",
        "        self.stage1 = ShuffleUnit(12,24)\n",
        "        self.stage2 = ShuffleUnit(24,32)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(32*7*7,64), nn.ReLU(), nn.Linear(64,10))\n",
        "    def forward(self,x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 6) MicroDenseNet\n",
        "class TinyDenseLayer(nn.Module):\n",
        "    def __init__(self, in_ch, growth):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_ch)\n",
        "        self.conv = nn.Conv2d(in_ch, growth, 3, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        return torch.cat([x, out], 1)\n",
        "class MicroDenseNet(nn.Module):\n",
        "    def __init__(self, growth=8):\n",
        "        super().__init__()\n",
        "        self.init = nn.Conv2d(1,12,3,padding=1,bias=False)\n",
        "        self.d1_1 = TinyDenseLayer(12,growth)\n",
        "        self.d1_2 = TinyDenseLayer(12+growth,growth)\n",
        "        self.d1_3 = TinyDenseLayer(12+2*growth,growth)\n",
        "        self.trans1 = nn.Sequential(nn.Conv2d(12+3*growth,20,1,bias=False), nn.AvgPool2d(2))\n",
        "        self.d2_1 = TinyDenseLayer(20,growth)\n",
        "        self.d2_2 = TinyDenseLayer(20+growth,growth)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(20+2*growth,10)\n",
        "    def forward(self,x):\n",
        "        x = self.init(x)\n",
        "        x = self.d1_1(x)\n",
        "        x = self.d1_2(x)\n",
        "        x = self.d1_3(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.d2_1(x)\n",
        "        x = self.d2_2(x)\n",
        "        x = self.pool(x).view(x.size(0),-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 7) TinyEffNet\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, exp=4, stride=1):\n",
        "        super().__init__()\n",
        "        mid = in_ch * exp\n",
        "        self.expand = nn.Sequential(nn.Conv2d(in_ch,mid,1,bias=False), nn.BatchNorm2d(mid), nn.ReLU())\n",
        "        self.dw = nn.Sequential(nn.Conv2d(mid,mid,3,stride=stride,padding=1,groups=mid,bias=False),\n",
        "                                nn.BatchNorm2d(mid), nn.ReLU())\n",
        "        self.project = nn.Sequential(nn.Conv2d(mid,out_ch,1,bias=False), nn.BatchNorm2d(out_ch))\n",
        "        self.use_res = (in_ch==out_ch and stride==1)\n",
        "    def forward(self,x):\n",
        "        out = self.expand(x)\n",
        "        out = self.dw(out)\n",
        "        out = self.project(out)\n",
        "        if self.use_res: return out + x\n",
        "        return out\n",
        "class TinyEffNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1,8,3,padding=1,bias=False)\n",
        "        self.mb1 = MBConv(8,16,exp=2)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.mb2 = MBConv(16,24,exp=2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.head = nn.Sequential(nn.Flatten(), nn.Linear(24*7*7,64), nn.ReLU(), nn.Linear(64,10))\n",
        "    def forward(self,x):\n",
        "        x = self.stem(x)\n",
        "        x = self.mb1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.mb2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# 8) FireNetMini\n",
        "class Fire(nn.Module):\n",
        "    def __init__(self,in_ch,sq_ch,exp_ch):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.Conv2d(in_ch,sq_ch,1)\n",
        "        self.expand1 = nn.Conv2d(sq_ch,exp_ch,1)\n",
        "        self.expand3 = nn.Conv2d(sq_ch,exp_ch,3,padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        s = self.relu(self.squeeze(x))\n",
        "        e = torch.cat([self.relu(self.expand1(s)),self.relu(self.expand3(s))],1)\n",
        "        return e\n",
        "class FireNetMini(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1,8,3,padding=1)\n",
        "        self.fire1 = Fire(8,4,4)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fire2 = Fire(8,6,6)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(12*7*7,64), nn.ReLU(), nn.Linear(64,10))\n",
        "    def forward(self,x):\n",
        "        x = self.stem(x)\n",
        "        x = self.fire1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.fire2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 9) TinyCNN\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,6,5), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(6,16,5), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*4*4,120), nn.ReLU(),\n",
        "            nn.Linear(120,84), nn.ReLU(),\n",
        "            nn.Linear(84,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# 10) MicroCNN (simpler than TinyCNN)\n",
        "class MicroCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,4,3), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(4,8,3), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8*5*5,32), nn.ReLU(),\n",
        "            nn.Linear(32,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# 11) DSConv\n",
        "class DSConv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,4,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(4,4,3,padding=1,groups=4), nn.ReLU(),\n",
        "            nn.Conv2d(4,8,1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8*14*14,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# 12) SqueezeTiny\n",
        "class SqueezeTiny(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.Conv2d(1,4,1)\n",
        "        self.expand = nn.Conv2d(4,8,3,padding=1)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(8*14*14,10))\n",
        "    def forward(self,x):\n",
        "        x = self.squeeze(x)\n",
        "        x = self.expand(x)\n",
        "        x = self.pool(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 13) TinyResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self,in_ch,out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch,out_ch,3,padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_ch,out_ch,3,padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += x\n",
        "        return self.relu(out)\n",
        "class TinyResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1,8,3,padding=1)\n",
        "        self.block1 = BasicBlock(8,8)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.block2 = BasicBlock(8,8)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(8*7*7,10))\n",
        "    def forward(self,x):\n",
        "        x = self.stem(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.pool2(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 14) TinyGoogLeNet\n",
        "class InceptionTiny(nn.Module):\n",
        "    def __init__(self,in_ch,out1,out3,out5):\n",
        "        super().__init__()\n",
        "        self.b1 = nn.Sequential(nn.Conv2d(in_ch,out1,1), nn.ReLU())\n",
        "        self.b3 = nn.Sequential(nn.Conv2d(in_ch,out3,3,padding=1), nn.ReLU())\n",
        "        self.b5 = nn.Sequential(nn.Conv2d(in_ch,out5,5,padding=2), nn.ReLU())\n",
        "    def forward(self,x):\n",
        "        return torch.cat([self.b1(x),self.b3(x),self.b5(x)],1)\n",
        "class TinyGoogLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv2d(1,16,3,padding=1)\n",
        "        self.inc1 = InceptionTiny(16,8,8,8)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.inc2 = InceptionTiny(24,12,12,12)\n",
        "        self.fc = nn.Linear(36*14*14,10)\n",
        "    def forward(self,x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inc1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.inc2(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# 15) LeNet\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1,6,5), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(6,16,5), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*4*4,120), nn.ReLU(),\n",
        "            nn.Linear(120,84), nn.ReLU(),\n",
        "            nn.Linear(84,10)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "# --------------------------\n",
        "# Model registry\n",
        "# --------------------------\n",
        "MODEL_REGISTRY = {\n",
        "    \"nanocnn\": NanoCNN,\n",
        "    \"picocnn\": PicoCNN,\n",
        "    \"minimobilenet\": MiniMobileNet,\n",
        "    \"mobilenano\": MobileNetNano,\n",
        "    \"tinyshufflenet\": TinyShuffleNet,\n",
        "    \"microdensenet\": MicroDenseNet,\n",
        "    \"tinyeffnet\": TinyEffNet,\n",
        "    \"firenetmini\": FireNetMini,\n",
        "    \"tinycnn\": TinyCNN,\n",
        "    \"microcnn\": MicroCNN,\n",
        "    \"dsconv\": DSConv,\n",
        "    \"squeezetiny\": SqueezeTiny,\n",
        "    \"tinyresnet\": TinyResNet,\n",
        "    \"tinygooglenet\": TinyGoogLeNet,\n",
        "    \"lenet\": LeNet\n",
        "}\n",
        "\n",
        "# --------------------------\n",
        "# Training helpers\n",
        "# --------------------------\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    n = 0\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()*X.size(0)\n",
        "        correct += (out.argmax(1)==y).sum().item()\n",
        "        n += X.size(0)\n",
        "    return total_loss/n, correct/n\n",
        "\n",
        "def evaluate(model, loader, loss_fn=None):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            out = model(X)\n",
        "            if loss_fn: losses += loss_fn(out,y).item()*X.size(0)\n",
        "            correct += (out.argmax(1)==y).sum().item()\n",
        "            total += X.size(0)\n",
        "    return (losses/total if loss_fn else None), correct/total\n",
        "\n",
        "# --------------------------\n",
        "# Run all models sequentially\n",
        "# --------------------------\n",
        "for name, cls in MODEL_REGISTRY.items():\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "    model = cls().to(device)\n",
        "    print(f\"Params: {count_params(model):,}\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    start_time = time.time()\n",
        "    for ep in range(1,EPOCHS+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, loss_fn)\n",
        "        print(f\"Epoch {ep}/{EPOCHS}: train_acc={tr_acc:.4f}, val_acc={val_acc:.4f}\")\n",
        "    print(f\"Final validation accuracy for {name}: {val_acc:.4f}\")\n",
        "    print(\"Time elapsed: %.2f sec\\n\" % (time.time()-start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmmJ_JX80ThN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}